{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import csv\n",
    "import unicodedata\n",
    "import warnings\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Clasificadores\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "#from sklearn.datasets import load_digits\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# #load data\n",
    "# df = pd.read_csv(\"./csv/0_500_union_salida_clasificada.csv\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "file_name=\"./csv/0_500_union_salida_clasificada.csv\"\n",
    "stop_file=\"custom_stopwords.txt\"    # Nombre del archivo de stopwords.\n",
    "\n",
    "# Regex para emoticones en texto.\n",
    "emoticons_str = r\"\"\"\n",
    "(?:\n",
    "    [:=;]               # Ojos\n",
    "    [oO\\-]?             # Nariz (optional)\n",
    "    [D\\)\\]\\(\\]/\\\\OpP]   # Bocas\n",
    ")\"\"\"\n",
    "\n",
    "# Regex para tokenizar correctamente.\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'(?:[\\w_]+)',                                        # Otras palabras\n",
    "    r'(?:\\S)'                                             # Cualquier otra cosa\n",
    "]\n",
    "\n",
    "# Se arman objetos para regular expresions.\n",
    "tokens_re   = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "# Se carga archivos de STOPWORDS\n",
    "with open(stop_file, newline='') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "    \n",
    "# Se abre archivo con tweets y se lo recorre    \n",
    "with open(file_name, newline='') as csvfile:\n",
    "\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\\\"')\n",
    "    header = next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "\n",
    "        y.append(row[0].lower())\n",
    "        \n",
    "        tweet = row[1].lower()                    # Se normaliza texto, todo a minusculas.\n",
    "        tweet = re.sub(r'@[a-z0-9_]+', '', tweet) # Se quitan menciones. @xxxxxxxx\n",
    "        tweet = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', '', tweet)\n",
    "        #tweet = re.sub(r'\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+', '', tweet)\n",
    "        #tweet = re.sub(r'(#)', '', tweet)\n",
    "        tweet = tweet.translate(str.maketrans('','', '.~Â¡!-_â€”#$%Â¿?:+-/Â°);(/\",*â€œâ€â€˜â€™'))\n",
    "        \n",
    "        # Manejo de emoticones.\n",
    "        emoticones = [\n",
    "                      ['ğŸ˜†',' risa'],['ğŸ˜‚','risa'],['ğŸ˜±',' asombro'],['ğŸ¥³',' felicidad'],['ğŸ’™',' amor'],['ğŸ˜',' amor'],\n",
    "                      ['ğŸ˜€',' sonreir'],['ğŸ‘',' ok'],['ğŸ¤”',' ok'],['ğŸŠ',' piÃ±ata'],['ğŸ™',' ojala'],['ğŸ’ªğŸ»',' fuerza'],\n",
    "                      ['ğŸ˜¡',' enojo'],['ğŸ˜›',' broma'],['ğŸ˜®',' asombro'],['ğŸ¤®',' desagradable'],['ğŸ‘ğŸ»',' aplauso'],\n",
    "                      ['ğŸ˜',' canchero'],['ğŸ˜©',' decepcion'],['ğŸ˜³',' verguenza'],['ğŸ˜Š',' contento'],['ğŸ˜¥',' triste'],\n",
    "                      ['ğŸ˜¤',' furioso'],['ğŸ–•',' enojo'],['ğŸ‘',' aplauso'],['ğŸ’ª',' fuerza'],['ğŸ¤¦â€','increible'],\n",
    "                      ['ğŸ™„','duda']\n",
    "                     ]\n",
    "        for emoji in emoticones:\n",
    "            tweet = tweet.replace(emoji[0], emoji[1])\n",
    "\n",
    "        # Manejo de acentos.\n",
    "        dict_acentos = [['Ã¡','a'],['Ã©','e'],['Ã­','i'],['Ã³','o'],['Ãº','u']]\n",
    "        for acento in dict_acentos:\n",
    "            tweet = tweet.replace(acento[0], acento[1])\n",
    "            \n",
    "        # Remueve letras repetidas y deja una sola.\n",
    "        for letra in ['a','e','i','o','u','s','c']:\n",
    "            pattern = letra + '{2,}'\n",
    "            tweet = re.sub(pattern, letra, tweet)\n",
    "        \n",
    "        tweet = tweet.translate(str.maketrans('','', 'ğŸ¥ğŸ§ğŸ³ğŸ–ğŸ›«ğŸ˜‘âœˆğŸ‡¦ğŸ‡·ğŸ‡µğŸ‡¾ğŸ‘‡ğŸ™ƒâ–¶ğŸ’»â–ºâ†’â¬‡ï¸ğŸ˜’ğŸ”«ğŸ”ğŸ”¥ğŸ’€ğŸš«ğŸ˜â™‚â¤â¤â¤ğŸ˜ğŸ‘Šï£¿ğŸ¤ğŸ»'))\n",
    "        tweet = re.sub(r'\\d+', '', tweet)         # Se quitan numeros.\n",
    "        tweet = tweet.strip()\n",
    "    \n",
    "        # Tokenizado\n",
    "        tokens = tokens_re.findall(tweet)\n",
    "\n",
    "        # Remocion de stopwords\n",
    "        tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "        s = ' '\n",
    "        X.append(s.join(tokens))\n",
    "        #print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando BayesSearchCV\n",
      "Naive Bayes\n",
      "precision 0.7291229481998011\n",
      "Parametros:  {'clf__alpha': 0.9, 'clf__fit_prior': False, 'tfidf__use_idf': True}\n",
      "recall 0.720558882235529\n",
      "Parametros:  {'clf__alpha': 0.9, 'clf__fit_prior': False, 'tfidf__use_idf': True}\n",
      "Random Forest\n",
      "precision 0.6464402986604582\n",
      "Parametros:  {'clf__criterion': 'entropy', 'clf__max_depth': 10, 'clf__min_samples_leaf': 10, 'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 2)}\n",
      "recall 0.5808383233532934\n",
      "Parametros:  {'clf__criterion': 'gini', 'clf__max_depth': 4, 'clf__min_samples_leaf': 10, 'tfidf__use_idf': False, 'vectorizer__ngram_range': (1, 1)}\n",
      "Neural Network\n",
      "precision 0.7038338419414977\n",
      "Parametros:  {'clf__alpha': 0.1, 'clf__hidden_layer_sizes': 6, 'clf__max_iter': 300, 'clf__random_state': 8, 'clf__solver': 'lbfgs', 'tfidf__use_idf': False}\n",
      "recall 0.7125748502994012\n",
      "Parametros:  {'clf__alpha': 0.1, 'clf__hidden_layer_sizes': 9, 'clf__max_iter': 300, 'clf__random_state': 8, 'clf__solver': 'lbfgs', 'tfidf__use_idf': True}\n",
      "SVM\n",
      "precision 0.7161773053624126\n",
      "Parametros:  {'clf__C': 122614.1597234793, 'clf__degree': 1, 'clf__gamma': 1.206507147572085, 'clf__kernel': 'linear', 'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 2)}\n",
      "recall 0.716566866267465\n",
      "Parametros:  {'clf__C': 25.793889856799204, 'clf__degree': 2, 'clf__gamma': 1.6961020458864662e-06, 'clf__kernel': 'linear', 'tfidf__use_idf': False, 'vectorizer__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "# --- Pipelines ---\n",
    "pipeline1 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', SVC())\n",
    "                     ])\n",
    "pipeline2 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', DecisionTreeClassifier())\n",
    "                     ])\n",
    "pipeline3 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MLPClassifier())\n",
    "                     ])\n",
    "pipeline4 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB())\n",
    "                     ])\n",
    "\n",
    "\n",
    "# --- Parameters ---\n",
    "param_svm = [{ 'vectorizer__ngram_range': [(1, 1),(1, 2),(2, 2)],\n",
    "               'tfidf__use_idf': (True, False),\n",
    "               'clf__kernel': ['linear', 'poly', 'rbf'],  # categorical parameter\n",
    "               'clf__gamma': (1e-6, 1e+1, 'log-uniform'),\n",
    "               'clf__C': (1e-6, 1e+6, 'log-uniform'),  \n",
    "               'clf__degree': (1, 8),  # integer valued parameter\n",
    "             }]\n",
    "param_tree = [{ 'vectorizer__ngram_range': [(1, 1),(1, 2),(2, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__criterion': ['gini', 'entropy'],\n",
    "                'clf__max_depth': [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n",
    "                'clf__min_samples_leaf': [1, 5, 10, 20]\n",
    "              }]    \n",
    "param_red =  [{ #'vectorizer__ngram_range': [(1, 2),(2, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__solver': ['lbfgs'], \n",
    "                'clf__max_iter': [300], \n",
    "                'clf__alpha': 10.0 ** -np.arange(1, 7), \n",
    "                'clf__hidden_layer_sizes':np.arange(5, 10), \n",
    "                'clf__random_state':[8] \n",
    "              }]\n",
    "param_nb  =  [{ #'vectorizer__ngram_range': [(1, 1),(1, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                #'clf__alpha': np.linspace(0.5, 1.5, 6),\n",
    "                'clf__alpha': (0.5,0.7,0.9,1.1,1.3,1.5),\n",
    "                'clf__fit_prior': [True, False] \n",
    "              }]\n",
    "\n",
    "\n",
    "# --- Scores ---\n",
    "scores = ['precision', 'recall']\n",
    "model = [\"Naive Bayes\", \"Random Forest\", \"Neural Network\", \"SVM\"]\n",
    "pips = [pipeline4, pipeline2, pipeline3, pipeline1] \n",
    "pars = [param_nb, param_tree, param_red, param_svm] \n",
    "\n",
    "\n",
    "# --- Cros Validate ---\n",
    "cvNum = 10\n",
    "\n",
    "print(\"Comenzando BayesSearchCV\")\n",
    "\n",
    "for i in range(len(pars)):\n",
    "    print(model[i])\n",
    "    for score in scores:\n",
    "        \n",
    "        gs_clf_svm = BayesSearchCV( pips[i], pars[i], cv=cvNum, scoring='%s_weighted' % score, n_jobs=-1, verbose=0, refit=False, n_iter=9)\n",
    "        gs_clf_svm = gs_clf_svm.fit(X, y)\n",
    "        \n",
    "        print(score, gs_clf_svm.best_score_)\n",
    "        print(\"Parametros: \", gs_clf_svm.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
